\section{Related Work} %\label{sec:extension_related}

\textbf{Polygonization}.
All available implementations for the polygonization procedure are built upon the JTS/GEOS implementation \cite{web:jts:polygonizer, web:geos:polygonizer}. While the JTS library is used in many modern distributed spatial analytics systems \cite{pandey_how_2021}, including Hadoop-GIS \cite{aji_hadoop-gis_2013}, SpatialHadoop  \cite{eldawy_spatialhadoop_2015}, GeoSpark \cite{yu_spatial_2018}, and SpatialSpark \cite{you_large-scale_2015}, the implementation of the polygonization algorithm \cite{web:jts:polygonizer} has not been extended to work in these distributed frameworks.

A data-parallel algorithm for polygonizing a collection of line segments represented by a data-parallel bucket PMR quadtree, a data-parallel $R$-tree, and a data-parallel $R^+$-tree was proposed in \cite{hoel_data-parallel_2003}.  The algorithm starts by partitioning the data using the given data-parallel structure (i.e., the PMR quadtree, the $R$-tree, or the $R^+$-tree), beginning the polygonization at the leaf nodes.  The polygonization starts by finding each line segment's left and right polygon  identifiers in each node. Then children nodes are merged into their direct parent node, at which redundancy is resolved. This procedure is recursively called until the root node is reached, where all line segments have their final left and right polygon identifiers assigned.

Each merging operation partitions the input data into a smaller number of partitions. At each iteration, the number of partitions decreases while the number of line segments entering and exiting each iteration remains constant.  This implies that at the last iteration, the whole input line segment dataset must be processed on only one partition at the root node level.  In the era of big data, where the use of commodity machines as worker nodes is common, this becomes a bottleneck when processing datasets of hundreds of millions of records on one machine.  While our work and the approach in \cite{hoel_data-parallel_2003} rely on iterative data re-partitioning, \cite{hoel_data-parallel_2003} uses a constant input to each iteration while significantly decreasing the number of partitions.  On the other hand, our input size decreases as the number of partitions decreases (thus avoiding processing the whole dataset on a single partition).
